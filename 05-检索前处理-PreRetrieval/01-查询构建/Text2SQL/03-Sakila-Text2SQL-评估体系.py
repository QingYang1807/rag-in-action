"""
Sakila Text2SQL è¯„ä¼°ä½“ç³»
ç”¨äºè¯„ä¼°Text2SQLæ¨¡å‹åœ¨Sakilaæ•°æ®åº“ä¸Šçš„æ€§èƒ½
"""

import json
import sqlite3
import os
import sys
from typing import List, Dict, Tuple, Any
from openai import OpenAI
import numpy as np
from difflib import SequenceMatcher
import re
from datetime import datetime
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'text2sql_evaluation_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class SakilaText2SQLEvaluator:
    """Sakilaæ•°æ®åº“Text2SQLè¯„ä¼°å™¨"""
    
    def __init__(self, 
                 db_path: str = "90-æ–‡æ¡£-Data/sakila.db",
                 test_data_path: str = "90-æ–‡æ¡£-Data/sakila/q2sql_pairs.json",
                 api_key: str = None,
                 model: str = "deepseek-chat",
                 base_url: str = "https://api.deepseek.com"):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            db_path: æ•°æ®åº“æ–‡ä»¶è·¯å¾„
            test_data_path: æµ‹è¯•æ•°æ®JSONæ–‡ä»¶è·¯å¾„
            api_key: APIå¯†é’¥
            model: ä½¿ç”¨çš„æ¨¡å‹åç§°
            base_url: APIåŸºç¡€URL
        """
        self.db_path = db_path
        self.test_data_path = test_data_path
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        if api_key:
            self.client = OpenAI(
                base_url=base_url,
                api_key=api_key
            )
        else:
            self.client = OpenAI(
                base_url=base_url,
                api_key=os.getenv("DEEPSEEK_API_KEY")
            )
        
        self.model = model
        self.schema_info = self._load_schema_info()
        self.test_data = self._load_test_data()
        
    def _load_schema_info(self) -> str:
        """åŠ è½½æ•°æ®åº“Schemaä¿¡æ¯"""
        schema_description = """
ä½ æ­£åœ¨è®¿é—®Sakilaç”µå½±ç§Ÿèµæ•°æ®åº“ï¼ŒåŒ…å«ä»¥ä¸‹ä¸»è¦è¡¨ï¼š

1. actorï¼ˆæ¼”å‘˜è¡¨ï¼‰
   - actor_id (INT): ä¸»é”®ï¼Œæ¼”å‘˜å”¯ä¸€æ ‡è¯†
   - first_name (VARCHAR): æ¼”å‘˜åå­—
   - last_name (VARCHAR): æ¼”å‘˜å§“æ°
   - last_update (TIMESTAMP): æœ€åæ›´æ–°æ—¶é—´

2. filmï¼ˆç”µå½±è¡¨ï¼‰
   - film_id (INT): ä¸»é”®ï¼Œç”µå½±å”¯ä¸€æ ‡è¯†
   - title (VARCHAR): ç”µå½±æ ‡é¢˜
   - description (TEXT): ç”µå½±æè¿°
   - release_year (YEAR): å‘è¡Œå¹´ä»½
   - language_id (INT): è¯­è¨€ID
   - rental_duration (INT): ç§Ÿèµæ—¶é•¿ï¼ˆå¤©ï¼‰
   - rental_rate (DECIMAL): ç§Ÿèµè´¹ç”¨
   - length (INT): ç”µå½±æ—¶é•¿ï¼ˆåˆ†é’Ÿï¼‰
   - rating (ENUM): åˆ†çº§(G,PG,PG-13,R,NC-17)
   - replacement_cost (DECIMAL): æ›¿æ¢æˆæœ¬

3. categoryï¼ˆç±»åˆ«è¡¨ï¼‰
   - category_id (INT): ä¸»é”®ï¼Œç±»åˆ«å”¯ä¸€æ ‡è¯†
   - name (VARCHAR): ç±»åˆ«åç§°

4. customerï¼ˆå®¢æˆ·è¡¨ï¼‰
   - customer_id (INT): ä¸»é”®ï¼Œå®¢æˆ·å”¯ä¸€æ ‡è¯†
   - store_id (INT): å•†åº—ID
   - first_name (VARCHAR): å®¢æˆ·åå­—
   - last_name (VARCHAR): å®¢æˆ·å§“æ°
   - email (VARCHAR): ç”µå­é‚®ç®±
   - address_id (INT): åœ°å€ID
   - active (BOOLEAN): æ˜¯å¦æ´»è·ƒ
   - create_date (DATETIME): åˆ›å»ºæ—¥æœŸ

5. rentalï¼ˆç§Ÿèµè¡¨ï¼‰
   - rental_id (INT): ä¸»é”®ï¼Œç§Ÿèµå”¯ä¸€æ ‡è¯†
   - rental_date (DATETIME): ç§Ÿèµæ—¥æœŸ
   - inventory_id (INT): åº“å­˜ID
   - customer_id (INT): å®¢æˆ·ID
   - return_date (DATETIME): å½’è¿˜æ—¥æœŸ
   - staff_id (INT): å‘˜å·¥ID

6. paymentï¼ˆæ”¯ä»˜è¡¨ï¼‰
   - payment_id (INT): ä¸»é”®ï¼Œæ”¯ä»˜å”¯ä¸€æ ‡è¯†
   - customer_id (INT): å®¢æˆ·ID
   - staff_id (INT): å‘˜å·¥ID
   - rental_id (INT): ç§ŸèµID
   - amount (DECIMAL): æ”¯ä»˜é‡‘é¢
   - payment_date (DATETIME): æ”¯ä»˜æ—¥æœŸ

7. inventoryï¼ˆåº“å­˜è¡¨ï¼‰
   - inventory_id (INT): ä¸»é”®ï¼Œåº“å­˜å”¯ä¸€æ ‡è¯†
   - film_id (INT): ç”µå½±ID
   - store_id (INT): å•†åº—ID

8. staffï¼ˆå‘˜å·¥è¡¨ï¼‰
   - staff_id (INT): ä¸»é”®ï¼Œå‘˜å·¥å”¯ä¸€æ ‡è¯†
   - first_name (VARCHAR): å‘˜å·¥åå­—
   - last_name (VARCHAR): å‘˜å·¥å§“æ°
   - address_id (INT): åœ°å€ID
   - store_id (INT): å•†åº—ID
   - email (VARCHAR): ç”µå­é‚®ç®±
   - active (BOOLEAN): æ˜¯å¦åœ¨èŒ

9. storeï¼ˆå•†åº—è¡¨ï¼‰
   - store_id (INT): ä¸»é”®ï¼Œå•†åº—å”¯ä¸€æ ‡è¯†
   - manager_staff_id (INT): ç»ç†å‘˜å·¥ID
   - address_id (INT): åœ°å€ID

å…³ç³»è¡¨ï¼š
- film_actor: ç”µå½±å’Œæ¼”å‘˜çš„å¤šå¯¹å¤šå…³ç³»
- film_category: ç”µå½±å’Œç±»åˆ«çš„å¤šå¯¹å¤šå…³ç³»
"""
        return schema_description
    
    def _load_test_data(self) -> List[Dict]:
        """åŠ è½½æµ‹è¯•æ•°æ®"""
        with open(self.test_data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return data
    
    def select_test_subset(self, count: int = 25) -> List[Dict]:
        """
        é€‰æ‹©æµ‹è¯•æ•°æ®å­é›†
        
        Args:
            count: è¦é€‰æ‹©çš„æµ‹è¯•æ ·æœ¬æ•°é‡
            
        Returns:
            é€‰ä¸­çš„æµ‹è¯•æ•°æ®å­é›†
        """
        if count >= len(self.test_data):
            return self.test_data
        
        # ç¡®ä¿é€‰æ‹©çš„æ•°æ®å…·æœ‰ä»£è¡¨æ€§ï¼ŒåŒ…å«ä¸åŒç±»å‹çš„SQLæ“ä½œ
        selected_data = []
        
        # åˆ†åˆ«é€‰æ‹©ä¸åŒç±»å‹çš„SQLè¯­å¥
        select_queries = [item for item in self.test_data if item['sql'].strip().upper().startswith('SELECT')]
        insert_queries = [item for item in self.test_data if item['sql'].strip().upper().startswith('INSERT')]
        update_queries = [item for item in self.test_data if item['sql'].strip().upper().startswith('UPDATE')]
        delete_queries = [item for item in self.test_data if item['sql'].strip().upper().startswith('DELETE')]
        
        # æŒ‰æ¯”ä¾‹é€‰æ‹©
        select_count = min(count // 2, len(select_queries))  # 50%ä¸ºSELECT
        insert_count = min(count // 6, len(insert_queries))  # çº¦17%ä¸ºINSERT
        update_count = min(count // 6, len(update_queries))  # çº¦17%ä¸ºUPDATE
        delete_count = count - select_count - insert_count - update_count  # å‰©ä½™ä¸ºDELETE
        delete_count = min(delete_count, len(delete_queries))
        
        selected_data.extend(select_queries[:select_count])
        selected_data.extend(insert_queries[:insert_count])
        selected_data.extend(update_queries[:update_count])
        selected_data.extend(delete_queries[:delete_count])
        
        # å¦‚æœè¿˜ä¸å¤Ÿï¼Œè¡¥å……SELECTæŸ¥è¯¢
        remaining = count - len(selected_data)
        if remaining > 0:
            remaining_selects = select_queries[select_count:select_count + remaining]
            selected_data.extend(remaining_selects)
        
        logger.info(f"é€‰æ‹©äº†{len(selected_data)}ä¸ªæµ‹è¯•æ ·æœ¬")
        logger.info(f"å…¶ä¸­SELECT: {select_count}, INSERT: {insert_count}, UPDATE: {update_count}, DELETE: {delete_count}")
        
        return selected_data[:count]
    
    def generate_sql(self, question: str) -> str:
        """
        ä½¿ç”¨LLMç”ŸæˆSQLæŸ¥è¯¢
        
        Args:
            question: è‡ªç„¶è¯­è¨€é—®é¢˜
            
        Returns:
            ç”Ÿæˆçš„SQLæŸ¥è¯¢è¯­å¥
        """
        prompt = f"""
ä»¥ä¸‹æ˜¯Sakilaç”µå½±ç§Ÿèµæ•°æ®åº“çš„ç»“æ„æè¿°ï¼š
{self.schema_info}

ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€é—®é¢˜å¦‚ä¸‹ï¼š
"{question}"

è¯·æ³¨æ„ï¼š
1. è¯·ä»”ç»†åˆ†æé—®é¢˜æ¶‰åŠçš„è¡¨å’Œå­—æ®µ
2. è€ƒè™‘è¡¨ä¹‹é—´çš„å…³è”å…³ç³»
3. è¯·åªè¿”å›SQLæŸ¥è¯¢è¯­å¥ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–è§£é‡Šã€æ³¨é‡Šæˆ–æ ¼å¼æ ‡è®°ï¼ˆå¦‚```sqlï¼‰
4. SQLè¯­å¥åº”è¯¥æ˜¯å¯æ‰§è¡Œçš„æ ‡å‡†SQL
"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªSQLä¸“å®¶ã€‚è¯·åªè¿”å›SQLæŸ¥è¯¢è¯­å¥ï¼Œä¸è¦åŒ…å«ä»»ä½•Markdownæ ¼å¼æˆ–å…¶ä»–è¯´æ˜ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0
            )
            
            # æ¸…ç†SQLè¯­å¥
            sql = response.choices[0].message.content.strip()
            sql = sql.replace('```sql', '').replace('```', '').strip()
            return sql
            
        except Exception as e:
            logger.error(f"ç”ŸæˆSQLæ—¶å‡ºé”™: {e}")
            return ""
    
    def normalize_sql(self, sql: str) -> str:
        """
        è§„èŒƒåŒ–SQLè¯­å¥ä»¥ä¾¿æ¯”è¾ƒ
        
        Args:
            sql: åŸå§‹SQLè¯­å¥
            
        Returns:
            è§„èŒƒåŒ–åçš„SQLè¯­å¥
        """
        # è½¬æ¢ä¸ºå°å†™
        sql = sql.lower().strip()
        
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        sql = re.sub(r'\s+', ' ', sql)
        
        # ç§»é™¤æœ«å°¾çš„åˆ†å·
        sql = sql.rstrip(';')
        
        # ç»Ÿä¸€å¼•å·
        sql = sql.replace('"', "'")
        
        return sql
    
    def sql_similarity(self, sql1: str, sql2: str) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªSQLè¯­å¥çš„ç›¸ä¼¼åº¦
        
        Args:
            sql1: ç¬¬ä¸€ä¸ªSQLè¯­å¥
            sql2: ç¬¬äºŒä¸ªSQLè¯­å¥
            
        Returns:
            ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        norm_sql1 = self.normalize_sql(sql1)
        norm_sql2 = self.normalize_sql(sql2)
        
        # ä½¿ç”¨åºåˆ—åŒ¹é…å™¨è®¡ç®—ç›¸ä¼¼åº¦
        similarity = SequenceMatcher(None, norm_sql1, norm_sql2).ratio()
        return similarity
    
    def execute_sql(self, sql: str) -> Tuple[bool, Any]:
        """
        æ‰§è¡ŒSQLè¯­å¥
        
        Args:
            sql: è¦æ‰§è¡Œçš„SQLè¯­å¥
            
        Returns:
            (æ˜¯å¦æˆåŠŸ, ç»“æœæˆ–é”™è¯¯ä¿¡æ¯)
        """
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # å¯¹äºSELECTè¯­å¥ï¼Œè¿”å›ç»“æœ
            if sql.strip().upper().startswith('SELECT'):
                cursor.execute(sql)
                results = cursor.fetchall()
                conn.close()
                return True, results
            else:
                # å¯¹äºDMLè¯­å¥ï¼Œè¿”å›affected rows
                cursor.execute(sql)
                affected_rows = cursor.rowcount
                conn.commit()
                conn.close()
                return True, affected_rows
                
        except Exception as e:
            if 'conn' in locals():
                conn.close()
            return False, str(e)
    
    def exact_match_score(self, predicted_sql: str, reference_sql: str) -> float:
        """
        è®¡ç®—ç²¾ç¡®åŒ¹é…åˆ†æ•°
        
        Args:
            predicted_sql: é¢„æµ‹çš„SQL
            reference_sql: å‚è€ƒSQL
            
        Returns:
            ç²¾ç¡®åŒ¹é…åˆ†æ•° (0 æˆ– 1)
        """
        norm_pred = self.normalize_sql(predicted_sql)
        norm_ref = self.normalize_sql(reference_sql)
        return 1.0 if norm_pred == norm_ref else 0.0
    
    def execution_accuracy(self, predicted_sql: str, reference_sql: str) -> float:
        """
        è®¡ç®—æ‰§è¡Œå‡†ç¡®åº¦ï¼ˆç»“æœæ˜¯å¦ç›¸åŒï¼‰
        
        Args:
            predicted_sql: é¢„æµ‹çš„SQL
            reference_sql: å‚è€ƒSQL
            
        Returns:
            æ‰§è¡Œå‡†ç¡®åº¦ (0 æˆ– 1)
        """
        pred_success, pred_result = self.execute_sql(predicted_sql)
        ref_success, ref_result = self.execute_sql(reference_sql)
        
        # å¦‚æœéƒ½æ‰§è¡ŒæˆåŠŸä¸”ç»“æœç›¸åŒï¼Œè¿”å›1
        if pred_success and ref_success:
            if pred_result == ref_result:
                return 1.0
            else:
                return 0.0
        # å¦‚æœéƒ½æ‰§è¡Œå¤±è´¥ï¼Œä¹Ÿç®—æ˜¯ä¸€è‡´çš„
        elif not pred_success and not ref_success:
            return 1.0
        else:
            return 0.0
    
    def evaluate_single_query(self, question: str, reference_sql: str) -> Dict[str, Any]:
        """
        è¯„ä¼°å•ä¸ªæŸ¥è¯¢
        
        Args:
            question: è‡ªç„¶è¯­è¨€é—®é¢˜
            reference_sql: å‚è€ƒSQL
            
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        # ç”ŸæˆSQL
        predicted_sql = self.generate_sql(question)
        
        # è®¡ç®—å„ç§æŒ‡æ ‡
        exact_match = self.exact_match_score(predicted_sql, reference_sql)
        similarity = self.sql_similarity(predicted_sql, reference_sql)
        exec_accuracy = self.execution_accuracy(predicted_sql, reference_sql)
        
        # æ£€æŸ¥SQLæ˜¯å¦å¯æ‰§è¡Œ
        is_executable, exec_result = self.execute_sql(predicted_sql)
        
        result = {
            'question': question,
            'reference_sql': reference_sql,
            'predicted_sql': predicted_sql,
            'exact_match': exact_match,
            'similarity': similarity,
            'execution_accuracy': exec_accuracy,
            'is_executable': is_executable,
            'execution_result': exec_result if is_executable else str(exec_result)
        }
        
        return result
    
    def evaluate_dataset(self, test_data: List[Dict]) -> Dict[str, Any]:
        """
        è¯„ä¼°æ•´ä¸ªæ•°æ®é›†
        
        Args:
            test_data: æµ‹è¯•æ•°æ®åˆ—è¡¨
            
        Returns:
            æ•´ä½“è¯„ä¼°ç»“æœ
        """
        results = []
        
        logger.info(f"å¼€å§‹è¯„ä¼°{len(test_data)}ä¸ªæµ‹è¯•æ ·æœ¬...")
        
        for i, item in enumerate(test_data):
            logger.info(f"è¯„ä¼°ç¬¬{i+1}/{len(test_data)}ä¸ªæ ·æœ¬...")
            
            result = self.evaluate_single_query(
                item['question'], 
                item['sql']
            )
            results.append(result)
            
            # æ‰“å°å½“å‰æ ·æœ¬çš„è¯„ä¼°ç»“æœ
            logger.info(f"é—®é¢˜: {item['question'][:50]}...")
            logger.info(f"å‚è€ƒSQL: {item['sql']}")
            logger.info(f"ç”ŸæˆSQL: {result['predicted_sql']}")
            logger.info(f"ç²¾ç¡®åŒ¹é…: {result['exact_match']}")
            logger.info(f"ç›¸ä¼¼åº¦: {result['similarity']:.3f}")
            logger.info(f"æ‰§è¡Œå‡†ç¡®åº¦: {result['execution_accuracy']}")
            logger.info(f"æ˜¯å¦å¯æ‰§è¡Œ: {result['is_executable']}")
            logger.info("-" * 80)
        
        # è®¡ç®—æ•´ä½“æŒ‡æ ‡
        exact_matches = [r['exact_match'] for r in results]
        similarities = [r['similarity'] for r in results]
        exec_accuracies = [r['execution_accuracy'] for r in results]
        executabilities = [r['is_executable'] for r in results]
        
        overall_results = {
            'total_samples': len(results),
            'exact_match_accuracy': np.mean(exact_matches),
            'average_similarity': np.mean(similarities),
            'execution_accuracy': np.mean(exec_accuracies),
            'execution_success_rate': np.mean(executabilities),
            'detailed_results': results
        }
        
        return overall_results
    
    def print_evaluation_summary(self, evaluation_results: Dict[str, Any]):
        """
        æ‰“å°è¯„ä¼°ç»“æœæ‘˜è¦
        
        Args:
            evaluation_results: è¯„ä¼°ç»“æœ
        """
        print("\n" + "="*60)
        print("            SAKILA TEXT2SQL è¯„ä¼°ç»“æœæ‘˜è¦")
        print("="*60)
        print(f"æ€»æµ‹è¯•æ ·æœ¬æ•°: {evaluation_results['total_samples']}")
        print(f"ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡: {evaluation_results['exact_match_accuracy']:.3f} ({evaluation_results['exact_match_accuracy']*100:.1f}%)")
        print(f"å¹³å‡SQLç›¸ä¼¼åº¦: {evaluation_results['average_similarity']:.3f}")
        print(f"æ‰§è¡Œå‡†ç¡®åº¦: {evaluation_results['execution_accuracy']:.3f} ({evaluation_results['execution_accuracy']*100:.1f}%)")
        print(f"SQLå¯æ‰§è¡Œç‡: {evaluation_results['execution_success_rate']:.3f} ({evaluation_results['execution_success_rate']*100:.1f}%)")
        print("="*60)
        
        # åˆ†æç»“æœ
        if evaluation_results['exact_match_accuracy'] >= 0.8:
            print("ğŸ‰ ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡ä¼˜ç§€ï¼")
        elif evaluation_results['exact_match_accuracy'] >= 0.6:
            print("ğŸ‘ ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡è‰¯å¥½")
        else:
            print("âš ï¸  ç²¾ç¡®åŒ¹é…å‡†ç¡®ç‡éœ€è¦æ”¹è¿›")
        
        if evaluation_results['execution_accuracy'] >= 0.8:
            print("ğŸ‰ æ‰§è¡Œå‡†ç¡®åº¦ä¼˜ç§€ï¼")
        elif evaluation_results['execution_accuracy'] >= 0.6:
            print("ğŸ‘ æ‰§è¡Œå‡†ç¡®åº¦è‰¯å¥½")
        else:
            print("âš ï¸  æ‰§è¡Œå‡†ç¡®åº¦éœ€è¦æ”¹è¿›")
        
        if evaluation_results['execution_success_rate'] >= 0.9:
            print("ğŸ‰ SQLå¯æ‰§è¡Œç‡ä¼˜ç§€ï¼")
        elif evaluation_results['execution_success_rate'] >= 0.7:
            print("ğŸ‘ SQLå¯æ‰§è¡Œç‡è‰¯å¥½")
        else:
            print("âš ï¸  SQLå¯æ‰§è¡Œç‡éœ€è¦æ”¹è¿›")
        
        print("="*60)
    
    def save_results(self, evaluation_results: Dict[str, Any], 
                    output_file: str = None):
        """
        ä¿å­˜è¯„ä¼°ç»“æœåˆ°æ–‡ä»¶
        
        Args:
            evaluation_results: è¯„ä¼°ç»“æœ
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        if output_file is None:
            output_file = f"sakila_text2sql_evaluation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(evaluation_results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹Sakila Text2SQLè¯„ä¼°...")
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = SakilaText2SQLEvaluator()
    
    # é€‰æ‹©æµ‹è¯•æ•°æ®å­é›† (25ä¸ªæ ·æœ¬)
    test_subset = evaluator.select_test_subset(25)
    
    # æ‰§è¡Œè¯„ä¼°
    results = evaluator.evaluate_dataset(test_subset)
    
    # æ‰“å°ç»“æœæ‘˜è¦
    evaluator.print_evaluation_summary(results)
    
    # ä¿å­˜ç»“æœ
    evaluator.save_results(results)
    
    print("âœ… è¯„ä¼°å®Œæˆï¼")

if __name__ == "__main__":
    main() 